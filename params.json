{
  "name": "Ift6266",
  "tagline": "Project repo for Deep Learning class at UdeM",
  "body": "### Project page\r\nWelcome to the deep learning project blog where a journal of the project's progress will be kept.\r\n\r\n### Before the project started\r\nThe first assignment was to implement an MLP. Since it was already done in the Machine Learning class last semester, I instead tried my hand for the first time at Theano and implemented an MLP with it.\r\n\r\n### Week of the 1st of February and 8th of February\r\nI will concentrate my efforts on the Vocal Synthesis project. I familiarized myself with LSTM, installed blocks/fuel and started to try to understand how to use a simple LSTM for this task using blocks' framework.\r\n\r\n### Week of the 15th of February\r\nLate update for that week as I was out of town for the weekend. This week I listened to Alex Graves' presentation on how to generate sample. I had trouble wrapping my head around everything. I will try to explain the basic idea of what I understand. First you have an RNN, on top of it you fit its output with a mixture of Gaussians from which you sample. These samples generate the next sequence. That's all good but what of the training process? How do you backpropagate a gradient to train the RNN? You could use the EM algorithm for the mixture phase, but then I don't quite understand how to train the RNN. An idea would be to do gradient descent on the likelihood as a cost function on the mixture of Gaussian, this would allow the gradient to go down the RNN. Okay, let's try to just do the mixture model.\r\n\r\nFirst I thought of a model like this **x** -> **h** -> _L_ <- **x** where **x** is the vector of input and **h** a shorthand for three independently computed quantities : _m_ mixture coefficient, _a_ averages of Gaussians and its _s_ standard deviations, finally _L_ the cost which is simply p(**x** ; _m_, _a_, _s_) = mixture of Gaussians (not sure I can type this equation on github) After asking around, I was told that this would not produce what I had hoped. It would instead learn a different distribution for every **x**. The good model would be to let _m_, _a_ and _s_ be free parameters like this : **x** -> _L_ <- **h**. Not sure I understand why...\r\n\r\n### Week of the 22th of February\r\nImplementation of the first model **x** -> **h** -> _L_ <- **x**. It did not work as expected! However, this is the one that I think will be used on top of the RNN so its not wasted time. \r\n\r\nAfter a lot of conversation with people knowledgeable in generative models, looks like this approach is not going to work and was a waste of time. I investigated the other idea of doing EM on the output of the RNN. It could work, the problem is that you don't have an analytical (meaning I failed at taking a paper and do the derivatives by hand) gradient on the EM part of the model and thus cannot train using backprop the RNN under it. The only thing that sounds promising at this stage is to try making the free parameters model on last week post work, however so far it just doesn't want to converge on the good distribution. This sounded so simple and elegant on Alex Graves' presentation, one small equation P(x_t|x_1:t)...\r\n\r\n### Week of the 29th of February\r\nI finally decided to try the model (the very first idea) with an RNN that would take x_t and spit out x_t+1. Now in order to compute anything meaningful for training, I need to give a gradient to the RNN. So on top of that, I will have a mixture of gaussians computed with EM. This will allow me to a) sample from it in the generative process and b) compute the likelihood during training from this mixture with the true x_t+1. Someone reading this will feel like I have been circling around the same ideas for three weeks, which is not quite false but not quite true, let's try to wrap it up a little and make it clearer : \r\n\r\nFrom Alex Graves' presentation I wanted to do this simple thing, a mixture of gaussian on top of an RNN. A choice arrived : how do you find the mixture? You can do gradient descent on the whole thing (RNN + GMM) or use something else like the EM algorithm for the GMM. The first solution would be the simplest as everything would live in theano so I tried it first. This solution required me to understand the differences between the two models proposed on the _Week of the 22th_ post. Then after much work, just the mixture model (**x** -> _L_ <- **h**) never converged with the fake data. Even worst, it occurred to me that if it would failed to converge on the fake data which are infinite (you can generate simple mixtures with numpy) it would for sure fare poorly on the real task. So then I moved to the second solution. I was reluctant to do this one as it would require dealing with an algorithm inside theano and outside theano and there are still open questions such as : Do you fit a new GMM on each sequence produced by the RNN or do you refit it with the already learned parameters from the past fits as the EM algorithm is very sensible to initialization? Also, the fit is after the RNN and the gradient for the RNN is after the fit, so there is no joint training of the model, they are in kind of independent phase and I am not sure if that is a good thing. You can't do a common trick of training on one epoch the RNN with the GMM fixed and the other epoch find the GMM with fixed parameters on the RNN because in this context it makes no sense. If you fix the GMM during an epoch, the likelihood for the gradient are taken from this GMM with the true sequence x_t+1 so it does not depend on whatever state the RNN produces! Looks like I don't get the whole picture yet but I decided to go with it anyway and I dealt with the inside/outisde theano problem which is explained on the next paragraph.\r\n\r\nThe challenge this week was to wrap my head around blocks and fuel. The problem is that the EM will be done by sklearn so outside theano and the likelihood will be computed inside theano. So first I tried to hack fuel by trying to pass the parameters of the mixtured as a DataStream channel, that didn't went far. Second I tried to find a way to give this to blocks, the reason is if I can work it out with blocks I will have access to all these already coded bricks like the LSTM. It took me a long time to understand but all I needed was the ComputationGraph class from which you can easily get the parameters of all your bricks. You can then build yourself a gradient and use theano.function with your own inputs and cost and boom magic. Handy trick to know.\r\n\r\nI have implemented these ideas with a fake data and a brick MLP, I just wanted for this week to get the code executing without error so next week comes the real dataset.\r\n\r\n### Week of the 7th of March\r\nThis week was much about programming, I implemented and fixed bugs for the model of last week with the true LSTM brick. On a few test run the likelihood seemed to be going down. I also added a few monitoring stuff since it is going to run on a GPU now. Let's see if at least the likelihood is going down through some epochs and after that, generation time. \r\n\r\nUnfortunately, at some point right before the change of epoch the likelihood suddenly goes to NaN. I have implemented a feature to save the model's parameters and spend most of my time debugging that.\r\n\r\n### Week of the 14th of March\r\nThis week was characterized by again going back to previous ideas and redoing a lot of things. I was not able to debug the NaN in the rnn_em model. I decided out of curiosity to go check the actual paper of Alex Graves' presentation and boom. I finally realized and understood what he meant by those mixtures of Gaussian and how to completely backpropagate on it using RNNs, no need to go out of this theano to do EM this time! I went back to my code and programmed this new model named lstm_gmm. What took me most of my time this week was that I had to figure out how to use all these Blocks' bricks and some numpy broadcasting magic but it all worked out. It is now running (10 times faster since it can stay in theano) and will start generating sound. Now I can run everything with blocks and use the MainLoop class so I might sadly scrap what I did last week.\r\n\r\n### Week of the 21st of March\r\nThe model now get stuck on one mixture by setting its probability to one and the others to zero. Doing so causes underflow on the GPU and make it crash. I have now put a sigmoid before the softmax of the mixutres to prevent a spike on one mixture. I have also noticed it does crazy stuff on the averages of the gaussians like setting one to -250 when the data is normalized between -1 and 1. To prevent that, I now have a 10*tanh before the averages so it is constrained between -10 and 10. I was able to not make the GPU crash with underflows with these additions but the likelihood got stuck very fast and it didn't train very well.\r\n\r\nA few days later I implemented the logadd function like we learnt in the machine learning class. logadd(a, b) = m + log(exp(a-m) + exp(b-m) where m = max(a, b). It is now way more numerically stable and the likelhood dropped by a lot! Still, the same kind of behaviour persists. It seems that I either give the model not enough capacity so the likelihood just oscillates because of variance in the data, giving me false impression of training but actually nothing is trained. Or, I give it too much capacity and it settles very quickly into a region and doesn't move from there. There can be three things here, first the right capacity for this learning task is in a very narrow region and would require extensive hyper-param tuning, or it is just not a good model for this task or my code is bugged. I spent quite a lot of time this week on the first option and running different experiments with different value of the lstm dimension or gaussians mixtures with me always falling into one of the two cases mentioned earlier. To rule out this third option I would have to port this model to exactly reproduce the handwriting experiment in the original paper. I ought to try that, but time is slowly running short...\r\n",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}