{"name":"Ift6266","tagline":"Project repo for Deep Learning class at UdeM","body":"### Project page\r\nWelcome to the deep learning project blog where a journal of the project's progress will be kept.\r\n\r\n### Before the project started\r\nThe first assignment was to implement an MLP. Since it was already done in the Machine Learning class last semester, I instead tried my hand for the first time at Theano and implemented an MLP with it.\r\n\r\n### Week of the 1st of February and 8th of February\r\nI will concentrate my efforts on the Vocal Synthesis project. I familiarized myself with LSTM, installed blocks/fuel and started to try to understand how to use a simple LSTM for this task using blocks' framework.\r\n\r\n### Week of the 15th of February\r\nLate update for that week as I was out of town for the weekend. This week I listened to Alex Graves' presentation on how to generate sample. I had trouble wrapping my head around everything. I will try to explain the basic idea of what I understand. First you have an RNN, on top of it you fit its output with a mixture of Gaussians from which you sample. These samples generate the next sequence. That's all good but what of the training process? How do you backpropagate a gradient to train the RNN? You could use the EM algorithm for the mixture phase, but then I don't quite understand how to train the RNN. An idea would be to do gradient descent on the likelihood as a cost function on the mixture of Gaussian, this would allow the gradient to go down the RNN. Okay, let's try to just do the mixture model.\r\n\r\nFirst I thought of a model like this **x** -> **h** -> _L_ <- **x** where **x** is the vector of input and **h** a shorthand for three independently computed quantities : _m_ mixture coefficient, _a_ averages of Gaussians and its _s_ standard deviations, finally _L_ the cost which is simply p(**x** ; _m_, _a_, _s_) = mixture of Gaussians (not sure I can type this equation on github) After asking around, I was told that this would not produce what I had hoped. It would instead learn a different distribution for every **x**. The good model would be to let _m_, _a_ and _s_ be free parameters like this : **x** -> _L_ <- **h**. Not sure I understand why...\r\n\r\n### Week if the 22th of February\r\nImplementation of the first model **x** -> **h** -> _L_ <- **x**. It did not work as expected! However, this is the one that will be used on top of the RNN so its not wasted time. ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}