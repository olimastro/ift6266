{"name":"Ift6266","tagline":"Project repo for Deep Learning class at UdeM","body":"### Project page\r\nWelcome to the deep learning project blog where a journal of the project's progress will be kept.\r\n\r\n### Before the project started\r\nThe first assignment was to implement an MLP. Since it was already done in the Machine Learning class last semester, I instead tried my hand for the first time at Theano and implemented an MLP with it.\r\n\r\n### Week of the 1st of February and 8th of February\r\nI will concentrate my efforts on the Vocal Synthesis project. I familiarized myself with LSTM, installed blocks/fuel and started to try to understand how to use a simple LSTM for this task using blocks' framework.\r\n\r\n### Week of the 15th of February\r\nLate update for that week as I was out of town for the weekend. This week I listened to Alex Graves' presentation on how to generate sample. I had trouble wrapping my head around everything. I will try to explain the basic idea of what I understand. First you have an RNN, on top of it you fit its output with a mixture of Gaussians from which you sample. These samples generate the next sequence. That's all good but what of the training process? How do you backpropagate a gradient to train the RNN? You could use the EM algorithm for the mixture phase, but then I don't quite understand how to train the RNN. An idea would be to do gradient descent on the likelihood as a cost function on the mixture of Gaussian, this would allow the gradient to go down the RNN. Okay, let's try to just do the mixture model.\r\n\r\nFirst I thought of a model like this **x** -> **h** -> _L_ <- **x** where **x** is the vector of input and **h** a shorthand for three independently computed quantities : _m_ mixture coefficient, _a_ averages of Gaussians and its _s_ standard deviations, finally _L_ the cost which is simply p(**x** ; _m_, _a_, _s_) = mixture of Gaussians (not sure I can type this equation on github) After asking around, I was told that this would not produce what I had hoped. It would instead learn a different distribution for every **x**. The good model would be to let _m_, _a_ and _s_ be free parameters like this : **x** -> _L_ <- **h**. Not sure I understand why...\r\n\r\n### Week of the 22th of February\r\nImplementation of the first model **x** -> **h** -> _L_ <- **x**. It did not work as expected! However, this is the one that I think will be used on top of the RNN so its not wasted time. \r\n\r\nAfter a lot of conversation with people knowledgeable in generative models, looks like this approach is not going to work and was a waste of time. I investigated the other idea of doing EM on the output of the RNN. It could work, the problem is that you don't have an analytical (meaning I failed at taking a paper and do the derivatives by hand) gradient on the EM part of the model and thus cannot train using backprop the RNN under it. The only thing that sounds promising at this stage is to try making the free parameters model on last week post work, however so far it just doesn't want to converge on the good distribution. This sounded so simple and elegant on Alex Graves' presentation, one small equation P(x_t|x_1:t)...","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}