<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Ift6266 by olimastro</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Ift6266</h1>
        <h2>Project repo for Deep Learning class at UdeM</h2>
        <a href="https://github.com/olimastro/ift6266" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h3>
<a id="project-page" class="anchor" href="#project-page" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project page</h3>

<p>Welcome to the deep learning project blog where a journal of the project's progress will be kept.</p>

<h3>
<a id="before-the-project-started" class="anchor" href="#before-the-project-started" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Before the project started</h3>

<p>The first assignment was to implement an MLP. Since it was already done in the Machine Learning class last semester, I instead tried my hand for the first time at Theano and implemented an MLP with it.</p>

<h3>
<a id="week-of-the-1st-of-february-and-8th-of-february" class="anchor" href="#week-of-the-1st-of-february-and-8th-of-february" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Week of the 1st of February and 8th of February</h3>

<p>I will concentrate my efforts on the Vocal Synthesis project. I familiarized myself with LSTM, installed blocks/fuel and started to try to understand how to use a simple LSTM for this task using blocks' framework.</p>

<h3>
<a id="week-of-the-15th-of-february" class="anchor" href="#week-of-the-15th-of-february" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Week of the 15th of February</h3>

<p>Late update for that week as I was out of town for the weekend. This week I listened to Alex Graves' presentation on how to generate sample. I had trouble wrapping my head around everything. I will try to explain the basic idea of what I understand. First you have an RNN, on top of it you fit its output with a mixture of Gaussians from which you sample. These samples generate the next sequence. That's all good but what of the training process? How do you backpropagate a gradient to train the RNN? You could use the EM algorithm for the mixture phase, but then I don't quite understand how to train the RNN. An idea would be to do gradient descent on the likelihood as a cost function on the mixture of Gaussian, this would allow the gradient to go down the RNN. Okay, let's try to just do the mixture model.</p>

<p>First I thought of a model like this <strong>x</strong> -&gt; <strong>h</strong> -&gt; <em>L</em> &lt;- <strong>x</strong> where <strong>x</strong> is the vector of input and <strong>h</strong> a shorthand for three independently computed quantities : <em>m</em> mixture coefficient, <em>a</em> averages of Gaussians and its <em>s</em> standard deviations, finally <em>L</em> the cost which is simply p(<strong>x</strong> ; <em>m</em>, <em>a</em>, <em>s</em>) = mixture of Gaussians (not sure I can type this equation on github) After asking around, I was told that this would not produce what I had hoped. It would instead learn a different distribution for every <strong>x</strong>. The good model would be to let <em>m</em>, <em>a</em> and <em>s</em> be free parameters like this : <strong>x</strong> -&gt; <em>L</em> &lt;- <strong>h</strong>. Not sure I understand why...</p>

<h3>
<a id="week-of-the-22th-of-february" class="anchor" href="#week-of-the-22th-of-february" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Week of the 22th of February</h3>

<p>Implementation of the first model <strong>x</strong> -&gt; <strong>h</strong> -&gt; <em>L</em> &lt;- <strong>x</strong>. It did not work as expected! However, this is the one that I think will be used on top of the RNN so its not wasted time. </p>

<p>After a lot of conversation with people knowledgeable in generative models, looks like this approach is not going to work and was a waste of time. I investigated the other idea of doing EM on the output of the RNN. It could work, the problem is that you don't have an analytical (meaning I failed at taking a paper and do the derivatives by hand) gradient on the EM part of the model and thus cannot train using backprop the RNN under it. The only thing that sounds promising at this stage is to try making the free parameters model on last week post work, however so far it just doesn't want to converge on the good distribution. This sounded so simple and elegant on Alex Graves' presentation, one small equation P(x_t|x_1:t)...</p>

<h3>
<a id="week-of-the-29th-of-february" class="anchor" href="#week-of-the-29th-of-february" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Week of the 29th of February</h3>

<p>I finally decided to try the model (the very first idea) with an RNN that would take x_t and spit out x_t+1. Now in order to compute anything meaningful for training, I need to give a gradient to the RNN. So on top of that, I will have a mixture of gaussians computed with EM. This will allow me to a) sample from it in the generative process and b) compute the likelihood during training from this mixture with the true x_t+1. Someone reading this will feel like I have been circling around the same ideas for three weeks, which is not quite false but not quite true, let's try to wrap it up a little and make it clearer : </p>

<p>From Alex Graves' presentation I wanted to do this simple thing, a mixture of gaussian on top of an RNN. A choice arrived : how do you find the mixture? You can do gradient descent on the whole thing (RNN + GMM) or use something else like the EM algorithm for the GMM. The first solution would be the simplest as everything would live in theano so I tried it first. This solution required me to understand the differences between the two models proposed on the <em>Week of the 22th</em> post. Then after much work, just the mixture model (<strong>x</strong> -&gt; <em>L</em> &lt;- <strong>h</strong>) never converged with the fake data. Even worst, it occurred to me that if it would failed to converge on the fake data which are infinite (you can generate simple mixtures with numpy) it would for sure fare poorly on the real task. So then I moved to the second solution. I was reluctant to do this one as it would require dealing with an algorithm inside theano and outside theano and there are still open questions such as : Do you fit a new GMM on each sequence produced by the RNN or do you refit it with the already learned parameters from the past fits as the EM algorithm is very sensible to initialization? Also, the fit is after the RNN and the gradient for the RNN is after the fit, so there is no joint training of the model, they are in kind of independent phase and I am not sure if that is a good thing. You can't do a common trick of training on one epoch the RNN with the GMM fixed and the other epoch find the GMM with fixed parameters on the RNN because in this context it makes no sense. If you fix the GMM during an epoch, the likelihood for the gradient are taken from this GMM with the true sequence x_t+1 so it does not depend on whatever state the RNN produces! Looks like I don't get the whole picture yet but I decided to go with it anyway and I dealt with the inside/outisde theano problem which is explained on the next paragraph.</p>

<p>The challenge this week was to wrap my head around blocks and fuel. The problem is that the EM will be done by sklearn so outside theano and the likelihood will be computed inside theano. So first I tried to hack fuel by trying to pass the parameters of the mixtured as a DataStream channel, that didn't went far. Second I tried to find a way to give this to blocks, the reason is if I can work it out with blocks I will have access to all these already coded bricks like the LSTM. It took me a long time to understand but all I needed was the ComputationGraph class from which you can easily get the parameters of all your bricks. You can then build yourself a gradient and use theano.function with your own inputs and cost and boom magic. Handy trick to know.</p>

<p>I have implemented these ideas with a fake data and a brick MLP, I just wanted for this week to get the code executing without error so next week comes the real dataset.</p>

<h3>
<a id="week-of-the-7th-of-march" class="anchor" href="#week-of-the-7th-of-march" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Week of the 7th of March</h3>

<p>This week was much about programming, I implemented and fixed bugs for the model of last week with the true LSTM brick. On a few test run the likelihood seemed to be going down. I also added a few monitoring stuff since it is going to run on a GPU now. Let's see if at least the likelihood is going down through some epochs and after that, generation time. </p>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/olimastro/ift6266/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/olimastro/ift6266/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/olimastro/ift6266"></a> is maintained by <a href="https://github.com/olimastro">olimastro</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
